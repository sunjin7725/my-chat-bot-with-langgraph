import os
import yaml

from typing import Union, Callable, List

from langchain_core.runnables import RunnableConfig
from langchain_core.runnables.graph import MermaidDrawMethod, NodeStyles
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.graph.state import CompiledStateGraph

from settings import secret_path


def graph_to_png(graph: CompiledStateGraph, output_file_path: str = "graph.png", xray: Union[int, bool] = False):
    """
    CompiledStateGraph ë¥¼ png íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ê·¸ë˜í”„ ê°ì²´ê°€ CompiledStateGraph ì¸ìŠ¤í„´ìŠ¤ì¸ ê²½ìš°
    í•´ë‹¹ ê·¸ë˜í”„ë¥¼ Mermaid í˜•ì‹ì˜ PNG ì´ë¯¸ì§€ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        graph: ì‹œê°í™”í•  ê·¸ë˜í”„ ê°ì²´. CompiledStateGraph ì¸ìŠ¤í„´ìŠ¤ì—¬ì•¼ í•©ë‹ˆë‹¤.
        ouput_file_path: png ì´ë¯¸ì§€ê°€ ìƒì„±ë  ê²½ë¡œ.
        xray: í‘œì‹œí•  ì„œë¸Œê·¸ë˜í”„.

    Returns:
        output_file_pathì— png íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

    Raises:
        Exception: ê·¸ë˜í”„ ì‹œê°í™” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ì˜ˆì™¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    try:
        if isinstance(graph, CompiledStateGraph):
            graph.get_graph(xray=xray).draw_mermaid_png(
                background_color="white",
                node_colors=NodeStyles(),
                output_file_path=output_file_path,
            )
    except Exception as e:
        print(f"[ERROR] Visualize Graph Error: {e}")


def get_role_from_messages(msg):
    if isinstance(msg, HumanMessage):
        return "user"
    elif isinstance(msg, AIMessage):
        return "assistant"
    else:
        return "assistant"


def messages_to_history(messages):
    return "\n".join([f"{get_role_from_messages(msg)}: {msg.content}" for msg in messages])


def stream_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str] = [],
    callback: Callable = None,
):
    """
    LangGraphì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph (CompiledStateGraph): ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs (dict): ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config (RunnableConfig): ì‹¤í–‰ ì„¤ì •
        node_names (List[str], optional): ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        callback (Callable, optional): ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜. ê¸°ë³¸ê°’ì€ None
            ì½œë°± í•¨ìˆ˜ëŠ” {"node": str, "content": str} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.

    Returns:
        None: í•¨ìˆ˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì¶œë ¥ë§Œ í•˜ê³  ë°˜í™˜ê°’ì€ ì—†ìŠµë‹ˆë‹¤.
    """
    prev_node = ""
    for chunk_msg, metadata in graph.stream(inputs, config, stream_mode="messages"):
        curr_node = metadata["langgraph_node"]

        # node_namesê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ ë…¸ë“œê°€ node_namesì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if not node_names or curr_node in node_names:
            # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰
            if callback:
                callback({"node": curr_node, "content": chunk_msg.content})
            # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥
            else:
                # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
                if curr_node != prev_node:
                    print("\n" + "=" * 50)
                    print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                    print("- " * 25)
                print(chunk_msg.content, end="", flush=True)

            prev_node = curr_node


def load_chat_model(model="gpt-4o-mini", temperature=None, stream=True):
    with open(secret_path) as f:
        secret = yaml.safe_load(f)
    model = ChatOpenAI(api_key=secret["openai"]["api_key"], model=model, temperature=temperature, streaming=stream)
    return model
